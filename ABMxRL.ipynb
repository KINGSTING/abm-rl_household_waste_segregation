{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641f70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ABM Framework (Mesa 2.1.0)\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import RandomActivation  # Correct import for Mesa 2.1.0\n",
    "from mesa.datacollection import DataCollector\n",
    "\n",
    "# --- GLOBAL MODEL PARAMETERS (Synthesized from Thesis Data) ---\n",
    "\n",
    "# Contextual Parameters (from Appendix E & Chapter 3.2)\n",
    "ANNUAL_SWM_BUDGET = 1_500_000 # â‚±1,500,000\n",
    "QUARTERLY_BUDGET = ANNUAL_SWM_BUDGET / 4 # â‚±375,000\n",
    "NUM_BARANGAYS = 7\n",
    "NUM_EPISODES = 10000 # Number of training runs for the RL agent\n",
    "STEPS_PER_EPISODE = 4 # 4 quarters per year (episode)\n",
    "\n",
    "# Barangay Data (Households, Local_Budget, Initial_Compliance)\n",
    "BARANGAY_DATA = {\n",
    "    \"LianganEast\": (608, 30000, 0.65), \n",
    "    \"Poblacion\": (700, 40000, 0.10),\n",
    "    \"Esperanza\": (550, 25000, 0.08),\n",
    "    \"Binuni\": (400, 20000, 0.05),\n",
    "    \"Demologan\": (450, 22000, 0.07),\n",
    "    \"Mati\": (500, 30000, 0.12),\n",
    "    \"Babalaya\": (350, 18000, 0.09),\n",
    "}\n",
    "\n",
    "# Cost Parameter Estimation (Chapter 3.2.4)\n",
    "COST_PER_ENFORCER_QUARTER = 30000 \n",
    "COST_PER_IEC_INTENSITY_UNIT = 10000 \n",
    "PAYOUT_PER_COMPLIANT_HOUSEHOLD = 50 \n",
    "\n",
    "# RL Reward Weights (Chapter 3.4.3)\n",
    "ALPHA_COMPLIANCE = 10.0 \n",
    "BETA_COST = 0.00001 \n",
    "GAMMA_DEFICIT = 50.0 \n",
    "\n",
    "# TPB Parameter Defaults (Chapter 3.2.1)\n",
    "TPB_WEIGHTS = {\n",
    "    'wA': 0.45,  # Attitude\n",
    "    'wSN': 0.35, # Subjective Norms \n",
    "    'wPBC': 0.20 # Perceived Behavioral Control\n",
    "}\n",
    "\n",
    "# Psychological Reactance Threshold (Chapter 3.2.1)\n",
    "REACTANCE_THRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ef277b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseholdAgent(Agent):\n",
    "    \"\"\"\n",
    "    Represents a household whose decision to segregate is based on the \n",
    "    Theory of Planned Behavior (TPB) and LGU policy utility.\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, initial_income, initial_edu_level, barangay_id):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.barangay_id = barangay_id\n",
    "        self.income = initial_income # Modulates sensitivity to financial policies\n",
    "        self.education = initial_edu_level\n",
    "        self.is_compliant = 0 # 1 if compliant, 0 otherwise\n",
    "        \n",
    "        # Internal TPB Constructs (A, SN, PBC - initialized based on synthetic data)\n",
    "        self.attitude = random.uniform(0.3, 0.7)\n",
    "        self.subj_norm = random.uniform(0.2, 0.6)\n",
    "        self.perc_b_control = random.uniform(0.4, 0.8)\n",
    "\n",
    "    def calculate_utility(self, fine_magnitude, incentive_payout, neighborhood_compliance):\n",
    "        \"\"\"\n",
    "        USegregate = (wA*A + wSN*SN + wPBC*PBC) + UPolicy(I) + epsilon\n",
    "        \"\"\"\n",
    "        wA, wSN, wPBC = TPB_WEIGHTS['wA'], TPB_WEIGHTS['wSN'], TPB_WEIGHTS['wPBC']\n",
    "        \n",
    "        # 1. Psychological Component \n",
    "        Psych_Utility = (wA * self.attitude) + \\\n",
    "                        (wSN * self.subj_norm) + \\\n",
    "                        (wPBC * self.perc_b_control)\n",
    "        \n",
    "        # 2. Policy Component (Objective Cost/Benefit) - UPolicy(I)\n",
    "        # Income sensitivity: Lower income -> higher sensitivity to financial policies.\n",
    "        income_sensitivity = 1.0 - (self.income / self.model.MAX_INCOME_PROXY) \n",
    "\n",
    "        # Fine disutility (negative)\n",
    "        Fine_Disutility = -fine_magnitude * income_sensitivity * self.model.ENFORCEMENT_PERCEPTION_RATE \n",
    "        \n",
    "        # Incentive utility (positive)\n",
    "        Incentive_Utility = incentive_payout * (1.0 - income_sensitivity) \n",
    "        \n",
    "        Policy_Utility = Fine_Disutility + Incentive_Utility\n",
    "\n",
    "        # 3. Stochastic Component (Epsilon)\n",
    "        epsilon = random.gauss(0, 0.1) \n",
    "\n",
    "        USegregate = Psych_Utility + Policy_Utility + epsilon\n",
    "        return USegregate\n",
    "\n",
    "    def update_tpb_constructs(self, policy_action, neighborhood_fined_rate, avg_compliance):\n",
    "        \"\"\"\n",
    "        Updates A, SN, and PBC based on LGU investment and social observation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Update Attitude (A) based on IEC and Reactance\n",
    "        self.attitude += 0.01 * policy_action['IEC'] * self.model.IEC_EFFECTIVENESS\n",
    "        if neighborhood_fined_rate > REACTANCE_THRESHOLD:\n",
    "            self.attitude -= 0.05 * (neighborhood_fined_rate - REACTANCE_THRESHOLD)\n",
    "            \n",
    "        # 2. Update Subjective Norm (SN) based on observed compliance\n",
    "        self.subj_norm = (0.8 * self.subj_norm) + (0.2 * avg_compliance)\n",
    "        \n",
    "        # 3. PBC is updated by LGU commitment\n",
    "        self.perc_b_control += 0.005 * (policy_action['IEC'] + policy_action['ENFORCE'])\n",
    "\n",
    "        # Clamp values\n",
    "        self.attitude = np.clip(self.attitude, 0, 1)\n",
    "        self.subj_norm = np.clip(self.subj_norm, 0, 1)\n",
    "        self.perc_b_control = np.clip(self.perc_b_control, 0, 1)\n",
    "\n",
    "    def step(self, fine_magnitude, incentive_payout, neighborhood_compliance):\n",
    "        \"\"\"Decision to segregate or not.\"\"\"\n",
    "        utility = self.calculate_utility(fine_magnitude, incentive_payout, neighborhood_compliance)\n",
    "\n",
    "        if utility > 0.5: \n",
    "            self.is_compliant = 1\n",
    "        else:\n",
    "            self.is_compliant = 0\n",
    "            \n",
    "        return self.is_compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05cfc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarangayAgent(Agent):\n",
    "    \"\"\"\n",
    "    Intermediate implementation layer. Receives LGU funds and manages local \n",
    "    implementation (enforcement, IEC, incentives).\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, name, households, local_budget, initial_compliance):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.name = name\n",
    "        self.households = households \n",
    "        self.local_budget = local_budget \n",
    "        self.current_compliance_rate = initial_compliance\n",
    "        self.last_enforcement_fined_count = 0\n",
    "        self.policy_allocation = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "        \n",
    "    def aggregate_and_report(self):\n",
    "        \"\"\"Aggregates household compliance and calculates local policy effects.\"\"\"\n",
    "        \n",
    "        # 1. Calculate Aggregate Compliance\n",
    "        compliant_count = sum(h.is_compliant for h in self.households)\n",
    "        self.current_compliance_rate = compliant_count / len(self.households)\n",
    "        \n",
    "        # 2. Calculate Enforcement/Fine Effectiveness\n",
    "        non_compliant_count = len(self.households) - compliant_count\n",
    "        \n",
    "        # Enforcement effectiveness is proportional to spending.\n",
    "        enforcement_staff = self.policy_allocation['ENFORCE'] / COST_PER_ENFORCER_QUARTER\n",
    "        \n",
    "        # Probability of being caught/fined\n",
    "        fined_prob = min(1.0, 0.05 * enforcement_staff) \n",
    "        \n",
    "        # Number of households fined this quarter\n",
    "        fined_count = sum(1 for _ in range(non_compliant_count) if random.random() < fined_prob)\n",
    "        self.last_enforcement_fined_count = fined_count\n",
    "\n",
    "        # Calculate incentive cost and successful enforcement cost\n",
    "        incentive_cost = compliant_count * PAYOUT_PER_COMPLIANT_HOUSEHOLD\n",
    "        enforcement_revenue = fined_count * self.model.LGU_FINE_MAGNITUDE # Revenue from fines\n",
    "        \n",
    "        return self.current_compliance_rate, self.last_enforcement_fined_count, incentive_cost, enforcement_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "041575f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGURLAgent(Agent):\n",
    "    \"\"\"\n",
    "    The Municipal LGU agent, which uses Q-learning to determine the optimal \n",
    "    budget allocation strategy (the action At).\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, rl_config):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.lr = rl_config['learning_rate']\n",
    "        self.gamma = rl_config['discount_factor']\n",
    "        self.epsilon = rl_config['epsilon']\n",
    "        self.action_space = rl_config['action_space'] \n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.action_space)))\n",
    "        self.last_state = None\n",
    "        self.last_action_idx = None\n",
    "        \n",
    "    def get_state(self, model):\n",
    "        \"\"\"\n",
    "        Defines the State Vector St = [Compliancet, AllocationPrev, BudgetRem, Quartert]\n",
    "        \"\"\"\n",
    "        # 1. Compliance (7 values)\n",
    "        compliance_vector = tuple(b.current_compliance_rate for b in model.barangays.values())\n",
    "        \n",
    "        # 2. Previous Allocation (Simplified placeholder for 21 values)\n",
    "        prev_alloc_iec = sum(b.policy_allocation['IEC'] for b in model.barangays.values())\n",
    "        prev_alloc_enforce = sum(b.policy_allocation['ENFORCE'] for b in model.barangays.values())\n",
    "        prev_alloc_incentive = sum(b.policy_allocation['INCENTIVE'] for b in model.barangays.values())\n",
    "        \n",
    "        # 3. Budget Remaining (Discretized)\n",
    "        budget_remaining = model.annual_budget_remaining\n",
    "        budget_bin = int(budget_remaining // 100000) # Discretize in â‚±100k bins\n",
    "        \n",
    "        # 4. Current Quarter\n",
    "        quarter = model.schedule.steps % STEPS_PER_EPISODE\n",
    "        \n",
    "        # The state is a tuple for use as a dictionary key in the Q-table\n",
    "        return compliance_vector + (budget_bin, quarter) \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy strategy to select one of the discrete policy options.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: Choose a random discrete action\n",
    "            action_idx = random.randrange(len(self.action_space))\n",
    "        else:\n",
    "            # Exploitation: Choose the best action from Q-table\n",
    "            action_idx = np.argmax(self.q_table[state])\n",
    "            \n",
    "        self.last_state = state\n",
    "        self.last_action_idx = action_idx\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def update_Q(self, new_state, reward):\n",
    "        \"\"\"Q-Learning update rule.\"\"\"\n",
    "        old_value = self.q_table[self.last_state][self.last_action_idx]\n",
    "        next_max = np.max(self.q_table[new_state])\n",
    "        \n",
    "        # Q(S, A) = Q(S, A) + LR * (Reward + Gamma * max(Q(S', a')) - Q(S, A))\n",
    "        new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[self.last_state][self.last_action_idx] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dc9b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWMModel(Model):\n",
    "    \"\"\"The main Agent-Based Model environment for SWM policy simulation.\"\"\"\n",
    "    def __init__(self, rl_config, fine_magnitude=500):\n",
    "        super().__init__()\n",
    "        self.schedule = RandomActivation(self)\n",
    "        self.running = True\n",
    "        \n",
    "        # Model-wide parameters\n",
    "        self.MAX_INCOME_PROXY = 50000 \n",
    "        self.ENFORCEMENT_PERCEPTION_RATE = 0.5 \n",
    "        self.IEC_EFFECTIVENESS = 0.05\n",
    "        self.LGU_FINE_MAGNITUDE = fine_magnitude \n",
    "\n",
    "        # Initialize ID counter safely, starting high to avoid low-ID collisions\n",
    "        self.current_id = 1000 \n",
    "        self.RL_AGENT_ID = 999 \n",
    "        \n",
    "        # RL setup\n",
    "        self.rl_agent = LGURLAgent(self.RL_AGENT_ID, self, rl_config) \n",
    "        self.schedule.add(self.rl_agent)\n",
    "        \n",
    "        self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "        self.quarterly_budget = QUARTERLY_BUDGET\n",
    "        \n",
    "        # Initialization\n",
    "        self.barangays = {}\n",
    "        self.household_agents = []\n",
    "        self._initialize_agents()\n",
    "        \n",
    "        # Data Collector setup\n",
    "        self.datacollector = DataCollector(\n",
    "            model_reporters={\"AvgCompliance\": lambda m: np.mean([b.current_compliance_rate for b in m.barangays.values()]),\n",
    "                             \"TotalCost\": lambda m: m.last_cost,\n",
    "                             \"RLReward\": lambda m: m.last_reward,\n",
    "                             \"BudgetAllocIEC\": lambda m: m.last_alloc['IEC'],\n",
    "                             \"BudgetAllocENFORCE\": lambda m: m.last_alloc['ENFORCE'],\n",
    "                             \"BudgetAllocINCENTIVE\": lambda m: m.last_alloc['INCENTIVE'],\n",
    "                            },\n",
    "            agent_reporters={} \n",
    "        )\n",
    "        self.last_cost = 0\n",
    "        self.last_reward = 0\n",
    "        self.last_alloc = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "\n",
    "    def _get_next_id(self):\n",
    "        \"\"\"Helper function to increment and return a guaranteed unique ID.\"\"\"\n",
    "        self.current_id += 1\n",
    "        return self.current_id\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        \"\"\"Initializes all Household and Barangay Agents using a reserved ID counter for stability.\"\"\"\n",
    "        \n",
    "        # Use a temporary counter for the low-ID Barangay Agents (IDs 1-7)\n",
    "        temp_id = 0\n",
    "        def get_next_temp_id():\n",
    "            nonlocal temp_id\n",
    "            temp_id += 1\n",
    "            return temp_id\n",
    "            \n",
    "        for name, data in BARANGAY_DATA.items():\n",
    "            num_h, local_budget, initial_compliance = data\n",
    "            \n",
    "            # Assign Barangay Agent a low, safe ID\n",
    "            barangay_agent = BarangayAgent(get_next_temp_id(), self, name, [], local_budget, initial_compliance)\n",
    "            self.schedule.add(barangay_agent)\n",
    "            self.barangays[name] = barangay_agent\n",
    "            \n",
    "            # Household Agents use the high ID counter (1001+)\n",
    "            for _ in range(num_h):\n",
    "                h_agent = HouseholdAgent(self._get_next_id(), self, random.uniform(10000, self.MAX_INCOME_PROXY), \n",
    "                                         random.choice([0, 1, 2]), name)\n",
    "                self.household_agents.append(h_agent)\n",
    "                barangay_agent.households.append(h_agent)\n",
    "                self.schedule.add(h_agent) # Still added for tracking, but stepped manually\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one quarter (one RL step).\"\"\"\n",
    "        \n",
    "        # --- 1. RL Agent makes Policy Decision (Action) and sets budget ---\n",
    "        current_state = self.rl_agent.get_state(self)\n",
    "        policy_vector = self.rl_agent.choose_action(current_state) \n",
    "        \n",
    "        total_spending = 0\n",
    "        fine_magnitude = self.LGU_FINE_MAGNITUDE\n",
    "        incentive_payout = PAYOUT_PER_COMPLIANT_HOUSEHOLD \n",
    "        \n",
    "        for i, b_name in enumerate(BARANGAY_DATA.keys()):\n",
    "            b_agent = self.barangays[b_name]\n",
    "            \n",
    "            # Slice the 21-D vector (3 values per barangay)\n",
    "            alloc_iec = policy_vector[i * 3 + 0]\n",
    "            alloc_enforce = policy_vector[i * 3 + 1]\n",
    "            alloc_incentive = policy_vector[i * 3 + 2]\n",
    "\n",
    "            b_agent.policy_allocation['IEC'] = alloc_iec\n",
    "            b_agent.policy_allocation['ENFORCE'] = alloc_enforce\n",
    "            b_agent.policy_allocation['INCENTIVE'] = alloc_incentive\n",
    "            \n",
    "            total_spending += alloc_iec + alloc_enforce + alloc_incentive\n",
    "\n",
    "        # ********** CRUCIAL FIX: MANUAL AGENT STEPPING **********\n",
    "        \n",
    "        total_compliance = 0\n",
    "        total_households = sum(len(b.households) for b in self.barangays.values())\n",
    "        total_incentive_cost = 0\n",
    "        \n",
    "        for b_agent in self.barangays.values():\n",
    "            neighborhood_compliance = b_agent.current_compliance_rate\n",
    "            \n",
    "            # 2a. Household Decisions (Explicitly call step with arguments)\n",
    "            for h_agent in b_agent.households:\n",
    "                h_agent.step(fine_magnitude, incentive_payout, neighborhood_compliance) \n",
    "            \n",
    "            # 2b. Barangay Aggregation and Reporting\n",
    "            compliance, fined_count, incentive_cost, enforcement_revenue = b_agent.aggregate_and_report()\n",
    "            \n",
    "            # 2c. Update total model metrics\n",
    "            total_compliance += compliance * len(b_agent.households)\n",
    "            total_incentive_cost += incentive_cost\n",
    "\n",
    "            # 2d. Update TPB attributes for the NEXT quarter\n",
    "            fined_rate = fined_count / len(b_agent.households) if len(b_agent.households) > 0 else 0\n",
    "            for h_agent in b_agent.households:\n",
    "                h_agent.update_tpb_constructs(b_agent.policy_allocation, fined_rate, compliance)\n",
    "\n",
    "\n",
    "        # --- 3. RL Agent Calculates Reward and Updates Q-Table ---\n",
    "        avg_compliance = total_compliance / total_households\n",
    "        actual_total_cost = total_spending + total_incentive_cost \n",
    "        budget_deficit = max(0, actual_total_cost - self.quarterly_budget)\n",
    "        \n",
    "        reward = (ALPHA_COMPLIANCE * avg_compliance) - \\\n",
    "                 (BETA_COST * actual_total_cost) - \\\n",
    "                 (GAMMA_DEFICIT * budget_deficit)\n",
    "        \n",
    "        new_state = self.rl_agent.get_state(self)\n",
    "        self.rl_agent.update_Q(new_state, reward)\n",
    "        \n",
    "        # --- 4. Advance Model Time Manually ---\n",
    "        self.schedule.steps += 1\n",
    "        \n",
    "        # Update annual budget remaining\n",
    "        self.annual_budget_remaining -= actual_total_cost\n",
    "        \n",
    "        # Reset budget and end episode if the year is over\n",
    "        if self.schedule.steps % STEPS_PER_EPISODE == 0:\n",
    "            self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "            \n",
    "        # Store metrics for data collection\n",
    "        self.last_cost = actual_total_cost\n",
    "        self.last_reward = reward\n",
    "        self.last_alloc = {'IEC': sum(b.policy_allocation['IEC'] for b in self.barangays.values()),\n",
    "                           'ENFORCE': sum(b.policy_allocation['ENFORCE'] for b in self.barangays.values()),\n",
    "                           'INCENTIVE': sum(b.policy_allocation['INCENTIVE'] for b in self.barangays.values())}\n",
    "\n",
    "        self.datacollector.collect(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4514aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous SWMModel instance deleted to clear scheduler.\n",
      "Model initialized. Target training episodes: 5000\n",
      "Total actions in the selected RL space: 27\n",
      "\n",
      "Ready to begin full training loop in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialization and Iteration Control ðŸ’¡\n",
    "\n",
    "# --- SAFETY CHECK: Define Critical Global Constants ---\n",
    "# Re-defining here ensures the functions below can access them reliably.\n",
    "MAX_EPISODES = 5000 # Example value\n",
    "QUARTERLY_BUDGET = 375000.0\n",
    "NUM_BARANGAYS = 7\n",
    "STEPS_PER_EPISODE = 4 \n",
    "\n",
    "# --- RL Action Space Definition and Configuration ---\n",
    "\n",
    "def generate_scenario_actions(iec_enabled, enforce_enabled, incentive_enabled):\n",
    "    \"\"\"Generates the 21-D action vectors for the specified regime.\"\"\"\n",
    "    spending_levels = [0, 50000, 100000]\n",
    "    actions = []\n",
    "    \n",
    "    for s_iec in spending_levels:\n",
    "        if not iec_enabled and s_iec > 0: continue\n",
    "        for s_enforce in spending_levels:\n",
    "            if not enforce_enabled and s_enforce > 0: continue\n",
    "            for s_incentive in spending_levels:\n",
    "                if not incentive_enabled and s_incentive > 0: continue\n",
    "                \n",
    "                total_alloc = s_iec + s_enforce + s_incentive\n",
    "                if total_alloc <= QUARTERLY_BUDGET: \n",
    "                    \n",
    "                    action_vector = []\n",
    "                    for _ in range(NUM_BARANGAYS):\n",
    "                        action_vector.extend([s_iec/NUM_BARANGAYS, s_enforce/NUM_BARANGAYS, s_incentive/NUM_BARANGAYS])\n",
    "                    \n",
    "                    actions.append(tuple(action_vector)) \n",
    "                    \n",
    "    return actions\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "HYBRID_ACTIONS = generate_scenario_actions(True, True, True)\n",
    "\n",
    "RL_CONFIG = {\n",
    "    'learning_rate': 0.1,\n",
    "    'discount_factor': 0.9,\n",
    "    'epsilon': 1.0, \n",
    "    'epsilon_decay': 0.9995,\n",
    "    'epsilon_min': 0.01,\n",
    "    'action_space': HYBRID_ACTIONS \n",
    "}\n",
    "\n",
    "# --- INITIALIZE THE MODEL FOR ITERATIVE RUNNING ---\n",
    "\n",
    "# Define and conditionally reset global model variables\n",
    "global global_model\n",
    "global training_results\n",
    "global current_episode\n",
    "\n",
    "if 'global_model' in globals():\n",
    "    try:\n",
    "        if global_model is not None:\n",
    "            del global_model \n",
    "            print(\"Previous SWMModel instance deleted to clear scheduler.\")\n",
    "    except NameError:\n",
    "        pass \n",
    "\n",
    "# Initialize global variables (Must be defined before Cell 7)\n",
    "global_model = SWMModel(RL_CONFIG)\n",
    "training_results = []\n",
    "current_episode = 0\n",
    "\n",
    "print(f\"Model initialized. Target training episodes: {MAX_EPISODES}\")\n",
    "print(f\"Total actions in the selected RL space: {len(HYBRID_ACTIONS)}\")\n",
    "\n",
    "\n",
    "# --- ITERATION CONTROL FUNCTION (Remains defined here) ---\n",
    "def run_single_episode(model, episode_number):\n",
    "    \"\"\"Runs one full episode (4 quarters) of the ABM simulation and updates the RL Q-table.\"\"\"\n",
    "    \n",
    "    if episode_number >= MAX_EPISODES:\n",
    "        return None, True\n",
    "\n",
    "    # Decay Epsilon \n",
    "    model.rl_agent.epsilon = max(\n",
    "        model.rl_agent.epsilon * RL_CONFIG['epsilon_decay'],\n",
    "        RL_CONFIG['epsilon_min']\n",
    "    )\n",
    "    \n",
    "    # Run 4 Quarterly Steps (ABM Simulation)\n",
    "    for _ in range(STEPS_PER_EPISODE):\n",
    "        model.step() \n",
    "\n",
    "    # Record Results\n",
    "    df = model.datacollector.get_model_vars_dataframe()\n",
    "    episode_data = df.iloc[-1]\n",
    "    \n",
    "    return episode_data, False\n",
    "\n",
    "# Output check: Status message for readiness\n",
    "print(\"\\nReady to begin full training loop in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e508a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop for 5000 episodes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b5c90f38ae40399c13f59b73f0c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Status] Episode 500 | Avg Comp: 0.1967 | Epsilon: 0.7788\n",
      "\n",
      "[Status] Episode 1000 | Avg Comp: 0.1965 | Epsilon: 0.6065\n",
      "\n",
      "[Status] Episode 1500 | Avg Comp: 0.1964 | Epsilon: 0.4723\n",
      "\n",
      "[Status] Episode 2000 | Avg Comp: 0.1964 | Epsilon: 0.3678\n",
      "\n",
      "[Status] Episode 2500 | Avg Comp: 0.1965 | Epsilon: 0.2864\n",
      "\n",
      "[Status] Episode 3000 | Avg Comp: 0.1959 | Epsilon: 0.2230\n",
      "\n",
      "[Status] Episode 3500 | Avg Comp: 0.1961 | Epsilon: 0.1737\n",
      "\n",
      "[Status] Episode 4000 | Avg Comp: 0.1966 | Epsilon: 0.1353\n",
      "\n",
      "[Status] Episode 4500 | Avg Comp: 0.1968 | Epsilon: 0.1053\n",
      "\n",
      "--- FULL TRAINING COMPLETE (5000 Episodes Run) ---\n",
      "Proceed to Cell 8 for final analysis and plotting.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Full Automatic Training Loop ðŸš€ (Runs 5000 Episodes)\n",
    "\n",
    "# Import tqdm for a nice progress bar\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# Check if the model is initialized (should be defined by Cell 6)\n",
    "if 'global_model' not in globals() or global_model is None:\n",
    "    print(\"ERROR: Model not initialized. Please run Cell 6 first.\")\n",
    "else:\n",
    "    print(f\"Starting training loop for {MAX_EPISODES} episodes...\")\n",
    "\n",
    "    # Start the loop from the current episode count up to MAX_EPISODES\n",
    "    start_episode = current_episode\n",
    "    \n",
    "    # Use tqdm to create the progress bar\n",
    "    for episode in tqdm(range(start_episode + 1, MAX_EPISODES + 1), initial=start_episode, total=MAX_EPISODES, desc=\"Training Progress\"):\n",
    "        \n",
    "        # 1. Update the global counter\n",
    "        current_episode = episode\n",
    "        \n",
    "        # 2. Run the episode\n",
    "        new_data, is_finished = run_single_episode(global_model, current_episode)\n",
    "\n",
    "        if is_finished:\n",
    "            break\n",
    "        \n",
    "        # 3. Store results\n",
    "        training_results.append(new_data)\n",
    "        \n",
    "        # 4. Optional: Print status periodically (e.g., every 100 episodes)\n",
    "        if episode % 100 == 0:\n",
    "            avg_compliance = new_data['AvgCompliance']\n",
    "            epsilon = global_model.rl_agent.epsilon\n",
    "            print(f\"\\n[Status] Episode {episode} | Avg Comp: {avg_compliance:.4f} | Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n--- FULL TRAINING COMPLETE ({current_episode} Episodes Run) ---\")\n",
    "    print(\"Proceed to Cell 8 for final analysis and plotting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
