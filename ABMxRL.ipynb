{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ABM Framework (Mesa 2.1.0)\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import RandomActivation  # Correct import for Mesa 2.1.0\n",
    "from mesa.datacollection import DataCollector\n",
    "\n",
    "# --- GLOBAL MODEL PARAMETERS (Synthesized from Thesis Data) ---\n",
    "\n",
    "# Contextual Parameters (from Appendix E & Chapter 3.2)\n",
    "ANNUAL_SWM_BUDGET = 1_500_000 # â‚±1,500,000\n",
    "QUARTERLY_BUDGET = ANNUAL_SWM_BUDGET / 4 # â‚±375,000\n",
    "NUM_BARANGAYS = 7\n",
    "NUM_EPISODES = 10000 # Number of training runs for the RL agent\n",
    "STEPS_PER_EPISODE = 4 # 4 quarters per year (episode)\n",
    "\n",
    "# Barangay Data (Households, Local_Budget, Initial_Compliance)\n",
    "BARANGAY_DATA = {\n",
    "    \"LianganEast\": (608, 30000, 0.65), \n",
    "    \"Poblacion\": (700, 40000, 0.10),\n",
    "    \"Esperanza\": (550, 25000, 0.08),\n",
    "    \"Binuni\": (400, 20000, 0.05),\n",
    "    \"Demologan\": (450, 22000, 0.07),\n",
    "    \"Mati\": (500, 30000, 0.12),\n",
    "    \"Babalaya\": (350, 18000, 0.09),\n",
    "}\n",
    "\n",
    "# Cost Parameter Estimation\n",
    "COST_PER_ENFORCER_QUARTER = 2000 \n",
    "COST_PER_IEC_INTENSITY_UNIT = 5000 \n",
    "PAYOUT_PER_COMPLIANT_HOUSEHOLD = 50 \n",
    "\n",
    "# RL Reward Weights (Chapter 3.4.3)\n",
    "ALPHA_COMPLIANCE = 100.0   # WAS 10.0 (Boost reward for high compliance)\n",
    "BETA_COST = 0.000001       # WAS 0.00001 (Make spending 10x cheaper penalty-wise)\n",
    "GAMMA_DEFICIT = 50.0       # Keep high to prevent bankruptcy\n",
    "\n",
    "# TPB Parameter Defaults (Chapter 3.2.1)\n",
    "TPB_WEIGHTS = {\n",
    "    'wA': 0.45,  # Attitude\n",
    "    'wSN': 0.35, # Subjective Norms \n",
    "    'wPBC': 0.20 # Perceived Behavioral Control\n",
    "}\n",
    "\n",
    "# Psychological Reactance Threshold (Chapter 3.2.1)\n",
    "REACTANCE_THRESHOLD = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef277b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: HOUSEHOLD AGENT (BALANCED) ---\n",
    "class HouseholdAgent(Agent):\n",
    "    def __init__(self, unique_id, model, initial_income, initial_edu_level, barangay_id):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.barangay_id = barangay_id\n",
    "        self.income = initial_income\n",
    "        self.education = initial_edu_level\n",
    "        self.is_compliant = 0\n",
    "        \n",
    "        # TPB Internal State\n",
    "        self.attitude = random.uniform(0.3, 0.7)\n",
    "        self.subj_norm = random.uniform(0.2, 0.6)\n",
    "        self.perc_b_control = random.uniform(0.4, 0.8)\n",
    "\n",
    "    def calculate_utility(self, fine_magnitude, incentive_payout, enforcement_risk):\n",
    "        wA, wSN, wPBC = TPB_WEIGHTS['wA'], TPB_WEIGHTS['wSN'], TPB_WEIGHTS['wPBC']\n",
    "        \n",
    "        # 1. Psychological Utility\n",
    "        # BOOST: Increased multiplier from 50 to 60. \n",
    "        # Avg person (0.5) now starts with 30 points (closer to the 50 target).\n",
    "        Raw_Psych = (wA * self.attitude) + (wSN * self.subj_norm) + (wPBC * self.perc_b_control)\n",
    "        Psych_Points = Raw_Psych * 60 \n",
    "        \n",
    "        # 2. Financial Sensitivity\n",
    "        sensitivity = 1.0 - (self.income / self.model.MAX_INCOME_PROXY)\n",
    "        \n",
    "        # 3. Policy Utility\n",
    "        \n",
    "        # Incentive: REMOVED the 0.5 dampener.\n",
    "        # Poor person (0.8 sens) gets 50 * 0.8 = 40 points. (Instant compliance if they get money)\n",
    "        Util_Incentive = incentive_payout * sensitivity \n",
    "        \n",
    "        # Fine: Added \"Perceived Risk\" Multiplier.\n",
    "        # People tend to overestimate police presence. \n",
    "        # If Risk is 5% (0.05), we multiply by 4 to simulate \"Fear Factor\".\n",
    "        # 500 * 0.05 * 4.0 * sensitivity = 100 * sensitivity points.\n",
    "        Util_Fine_Avoidance = fine_magnitude * enforcement_risk * sensitivity * 4.0\n",
    "        \n",
    "        # Total Score\n",
    "        Total_Utility = Psych_Points + Util_Incentive + Util_Fine_Avoidance\n",
    "        \n",
    "        return Total_Utility\n",
    "\n",
    "    def update_tpb_constructs(self, policy_action, neighborhood_fined_rate, avg_compliance):\n",
    "        self.attitude += 0.01 * (policy_action['IEC'] / 20000) * self.model.IEC_EFFECTIVENESS\n",
    "        if neighborhood_fined_rate > REACTANCE_THRESHOLD:\n",
    "            self.attitude -= 0.05 \n",
    "        self.subj_norm = (0.9 * self.subj_norm) + (0.1 * avg_compliance)\n",
    "        self.perc_b_control += 0.005 * ((policy_action['IEC'] + policy_action['ENFORCE'])/50000)\n",
    "\n",
    "        self.attitude = np.clip(self.attitude, 0, 1)\n",
    "        self.subj_norm = np.clip(self.subj_norm, 0, 1)\n",
    "        self.perc_b_control = np.clip(self.perc_b_control, 0, 1)\n",
    "\n",
    "    def step(self, fine_magnitude, incentive_payout, enforcement_risk):\n",
    "        utility = self.calculate_utility(fine_magnitude, incentive_payout, enforcement_risk)\n",
    "        \n",
    "        # THRESHOLD: 50 Points\n",
    "        self.is_compliant = 1 if utility > 50 else 0\n",
    "        return self.is_compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05cfc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarangayAgent(Agent):\n",
    "    \"\"\"\n",
    "    Intermediate implementation layer. Receives LGU funds and manages local \n",
    "    implementation (enforcement, IEC, incentives).\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, name, households, local_budget, initial_compliance):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.name = name\n",
    "        self.households = households \n",
    "        self.local_budget = local_budget \n",
    "        self.current_compliance_rate = initial_compliance\n",
    "        self.last_enforcement_fined_count = 0\n",
    "        self.policy_allocation = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "        \n",
    "    def aggregate_and_report(self):\n",
    "        \"\"\"Aggregates household compliance and calculates local policy effects.\"\"\"\n",
    "        \n",
    "        # 1. Calculate Aggregate Compliance\n",
    "        compliant_count = sum(h.is_compliant for h in self.households)\n",
    "        self.current_compliance_rate = compliant_count / len(self.households)\n",
    "        \n",
    "        # 2. Calculate Enforcement/Fine Effectiveness\n",
    "        non_compliant_count = len(self.households) - compliant_count\n",
    "        \n",
    "        # Enforcement effectiveness is proportional to spending.\n",
    "        enforcement_staff = self.policy_allocation['ENFORCE'] / COST_PER_ENFORCER_QUARTER\n",
    "        \n",
    "        # Probability of being caught/fined\n",
    "        fined_prob = min(1.0, 0.05 * enforcement_staff) \n",
    "        \n",
    "        # Number of households fined this quarter\n",
    "        fined_count = sum(1 for _ in range(non_compliant_count) if random.random() < fined_prob)\n",
    "        self.last_enforcement_fined_count = fined_count\n",
    "\n",
    "        # Calculate incentive cost and successful enforcement cost\n",
    "        incentive_cost = compliant_count * PAYOUT_PER_COMPLIANT_HOUSEHOLD\n",
    "        enforcement_revenue = fined_count * self.model.LGU_FINE_MAGNITUDE # Revenue from fines\n",
    "        \n",
    "        return self.current_compliance_rate, self.last_enforcement_fined_count, incentive_cost, enforcement_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041575f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 4: RL AGENT ---\n",
    "class LGURLAgent(Agent):\n",
    "    def __init__(self, unique_id, model, rl_config):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.lr = rl_config['learning_rate']\n",
    "        self.gamma = rl_config['discount_factor']\n",
    "        self.epsilon = rl_config['epsilon']\n",
    "        self.action_space = rl_config['action_space'] \n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.action_space)))\n",
    "        self.last_state = None\n",
    "        self.last_action_idx = None\n",
    "        \n",
    "    def get_state(self, model):\n",
    "        # SIMPLIFIED STATE: Global Average Compliance (0-20 scale)\n",
    "        avg_compliance = np.mean([b.current_compliance_rate for b in model.barangays.values()])\n",
    "        compliance_bin = int(avg_compliance * 20) # 20% -> 4\n",
    "        \n",
    "        budget_bin = int(model.annual_budget_remaining // 100000) \n",
    "        quarter = model.schedule.steps % STEPS_PER_EPISODE\n",
    "        \n",
    "        # Returns a tuple like (4, 15, 1) - Easy to learn!\n",
    "        return (compliance_bin, budget_bin, quarter) \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action_idx = random.randrange(len(self.action_space))\n",
    "        else:\n",
    "            action_idx = np.argmax(self.q_table[state])\n",
    "        self.last_state = state\n",
    "        self.last_action_idx = action_idx\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def update_Q(self, new_state, reward):\n",
    "        old_value = self.q_table[self.last_state][self.last_action_idx]\n",
    "        next_max = np.max(self.q_table[new_state])\n",
    "        new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[self.last_state][self.last_action_idx] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc9b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 5: SWM MODEL ---\n",
    "class SWMModel(Model):\n",
    "    def __init__(self, rl_config, fine_magnitude=500):\n",
    "        super().__init__()\n",
    "        self.schedule = RandomActivation(self)\n",
    "        self.running = True\n",
    "        \n",
    "        self.MAX_INCOME_PROXY = 50000 \n",
    "        self.ENFORCEMENT_PERCEPTION_RATE = 0.5 \n",
    "        self.IEC_EFFECTIVENESS = 0.05\n",
    "        self.LGU_FINE_MAGNITUDE = fine_magnitude \n",
    "\n",
    "        self.current_id = 1000 \n",
    "        self.RL_AGENT_ID = 999 \n",
    "        \n",
    "        self.rl_agent = LGURLAgent(self.RL_AGENT_ID, self, rl_config) \n",
    "        self.schedule.add(self.rl_agent)\n",
    "        \n",
    "        self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "        self.quarterly_budget = QUARTERLY_BUDGET\n",
    "        \n",
    "        self.barangays = {}\n",
    "        self.household_agents = []\n",
    "        self._initialize_agents()\n",
    "        \n",
    "        self.datacollector = DataCollector(\n",
    "            model_reporters={\n",
    "                \"AvgCompliance\": lambda m: np.mean([b.current_compliance_rate for b in m.barangays.values()]),\n",
    "                \"Total_Spent_This_Q\": lambda m: m.last_cost,\n",
    "                \"Allocated_IEC\": lambda m: m.last_alloc['IEC'],\n",
    "                \"Allocated_Enforce\": lambda m: m.last_alloc['ENFORCE'],\n",
    "                \"Allocated_Incentive\": lambda m: m.last_alloc['INCENTIVE'],\n",
    "                \"RLReward\": lambda m: m.last_reward\n",
    "            }\n",
    "        )\n",
    "        self.last_cost = 0\n",
    "        self.last_reward = 0\n",
    "        self.last_alloc = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "\n",
    "    def _get_next_id(self):\n",
    "        self.current_id += 1\n",
    "        return self.current_id\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        temp_id = 0\n",
    "        def get_next_temp_id():\n",
    "            nonlocal temp_id\n",
    "            temp_id += 1\n",
    "            return temp_id\n",
    "            \n",
    "        for name, data in BARANGAY_DATA.items():\n",
    "            num_h, local_budget, initial_compliance = data\n",
    "            barangay_agent = BarangayAgent(get_next_temp_id(), self, name, [], local_budget, initial_compliance)\n",
    "            self.schedule.add(barangay_agent)\n",
    "            self.barangays[name] = barangay_agent\n",
    "            \n",
    "            for _ in range(num_h):\n",
    "                h_agent = HouseholdAgent(self._get_next_id(), self, random.uniform(10000, self.MAX_INCOME_PROXY), \n",
    "                                         random.choice([0, 1, 2]), name)\n",
    "                self.household_agents.append(h_agent)\n",
    "                barangay_agent.households.append(h_agent)\n",
    "                self.schedule.add(h_agent) \n",
    "\n",
    "    def step(self):\n",
    "        # 1. RL Agent Decides\n",
    "        current_state = self.rl_agent.get_state(self)\n",
    "        policy_vector = self.rl_agent.choose_action(current_state) \n",
    "        \n",
    "        total_spending = 0\n",
    "        for i, b_name in enumerate(BARANGAY_DATA.keys()):\n",
    "            b_agent = self.barangays[b_name]\n",
    "            \n",
    "            # Update Budgets\n",
    "            b_agent.policy_allocation['IEC'] = policy_vector[i * 3 + 0]\n",
    "            b_agent.policy_allocation['ENFORCE'] = policy_vector[i * 3 + 1]\n",
    "            b_agent.policy_allocation['INCENTIVE'] = policy_vector[i * 3 + 2]\n",
    "            total_spending += sum(b_agent.policy_allocation.values())\n",
    "\n",
    "        # 2. Households React\n",
    "        total_compliance = 0\n",
    "        total_households = sum(len(b.households) for b in self.barangays.values())\n",
    "        total_incentive_cost = 0\n",
    "        \n",
    "        for b_agent in self.barangays.values():\n",
    "            \n",
    "            # --- CONNECTION FIX: CALCULATE RISK ---\n",
    "            # Budget -> Staff -> Probability\n",
    "            enforcers = b_agent.policy_allocation['ENFORCE'] / COST_PER_ENFORCER_QUARTER\n",
    "            risk_prob = min(1.0, 0.05 * enforcers) \n",
    "            \n",
    "            for h_agent in b_agent.households:\n",
    "                # PASS RISK PROBABILITY to Household\n",
    "                h_agent.step(self.LGU_FINE_MAGNITUDE, PAYOUT_PER_COMPLIANT_HOUSEHOLD, risk_prob) \n",
    "            \n",
    "            compliance, fined_count, incentive_cost, revenue = b_agent.aggregate_and_report()\n",
    "            total_compliance += compliance * len(b_agent.households)\n",
    "            total_incentive_cost += incentive_cost\n",
    "\n",
    "            fined_rate = fined_count / len(b_agent.households) if len(b_agent.households) > 0 else 0\n",
    "            for h_agent in b_agent.households:\n",
    "                h_agent.update_tpb_constructs(b_agent.policy_allocation, fined_rate, compliance)\n",
    "\n",
    "        # 3. Reward Calculation\n",
    "        avg_compliance = total_compliance / total_households\n",
    "        actual_total_cost = total_spending + total_incentive_cost \n",
    "        budget_deficit = max(0, actual_total_cost - self.quarterly_budget)\n",
    "        \n",
    "        reward = (ALPHA_COMPLIANCE * avg_compliance) - (BETA_COST * actual_total_cost) - (GAMMA_DEFICIT * budget_deficit)\n",
    "        \n",
    "        new_state = self.rl_agent.get_state(self)\n",
    "        self.rl_agent.update_Q(new_state, reward)\n",
    "        \n",
    "        # 4. Updates\n",
    "        self.schedule.steps += 1\n",
    "        self.annual_budget_remaining -= actual_total_cost\n",
    "        if self.schedule.steps % STEPS_PER_EPISODE == 0:\n",
    "            self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "            \n",
    "        self.last_cost = actual_total_cost\n",
    "        self.last_reward = reward\n",
    "        self.last_alloc = {\n",
    "            'IEC': sum(b.policy_allocation['IEC'] for b in self.barangays.values()),\n",
    "            'ENFORCE': sum(b.policy_allocation['ENFORCE'] for b in self.barangays.values()),\n",
    "            'INCENTIVE': sum(b.policy_allocation['INCENTIVE'] for b in self.barangays.values())\n",
    "        }\n",
    "        self.datacollector.collect(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4514aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Target training episodes: 5000\n",
      "Total actions in the selected RL space: 30\n",
      "\n",
      "Ready to begin full training loop in the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Initialization and Iteration Control ðŸ’¡\n",
    "\n",
    "# --- SAFETY CHECK: Define Critical Global Constants ---\n",
    "# Re-defining here ensures the functions below can access them reliably.\n",
    "MAX_EPISODES = 5000 # Example value\n",
    "QUARTERLY_BUDGET = 375000.0\n",
    "NUM_BARANGAYS = 7\n",
    "STEPS_PER_EPISODE = 4 \n",
    "\n",
    "# --- RL Action Space Definition and Configuration ---\n",
    "\n",
    "def generate_scenario_actions(iec_enabled, enforce_enabled, incentive_enabled):\n",
    "    \"\"\"Generates baseline and targeted (Heuristic) actions.\"\"\"\n",
    "    actions = []\n",
    "    # Baseline Options (Equal Split)\n",
    "    spending_levels = [0, 50000, 100000]\n",
    "    for s_iec in spending_levels:\n",
    "        for s_enforce in spending_levels:\n",
    "            for s_incentive in spending_levels:\n",
    "                total_alloc = s_iec + s_enforce + s_incentive\n",
    "                if total_alloc <= QUARTERLY_BUDGET: \n",
    "                    action_vector = []\n",
    "                    for _ in range(NUM_BARANGAYS):\n",
    "                        action_vector.extend([s_iec/NUM_BARANGAYS, s_enforce/NUM_BARANGAYS, s_incentive/NUM_BARANGAYS])\n",
    "                    actions.append(tuple(action_vector))\n",
    "\n",
    "    # Heuristic Options (Targeted)\n",
    "    if enforce_enabled: # Iron Fist\n",
    "        iron_fist = []\n",
    "        for _ in range(NUM_BARANGAYS): iron_fist.extend([0, 200000/NUM_BARANGAYS, 0])\n",
    "        actions.append(tuple(iron_fist))\n",
    "    if iec_enabled: # Educator\n",
    "        educator = []\n",
    "        for _ in range(NUM_BARANGAYS): educator.extend([200000/NUM_BARANGAYS, 0, 0])\n",
    "        actions.append(tuple(educator))\n",
    "    if incentive_enabled: # Populist\n",
    "        populist = []\n",
    "        for _ in range(NUM_BARANGAYS): populist.extend([0, 0, 200000/NUM_BARANGAYS])\n",
    "        actions.append(tuple(populist))\n",
    "    return actions\n",
    "\n",
    "# Generate the actions first \n",
    "HYBRID_ACTIONS = generate_scenario_actions(True, True, True)\n",
    "\n",
    "RL_CONFIG = {\n",
    "    'learning_rate': 0.1, \n",
    "    'discount_factor': 0.9, \n",
    "    'epsilon': 1.0, \n",
    "    # CHANGE 1: Faster Decay (0.9995 -> 0.99)\n",
    "    # This makes the agent \"grow up\" in 500 episodes instead of 5000.\n",
    "    'epsilon_decay': 0.99, \n",
    "    'epsilon_min': 0.01, \n",
    "    'action_space': HYBRID_ACTIONS \n",
    "}\n",
    "\n",
    "# --- INITIALIZE THE MODEL FOR ITERATIVE RUNNING ---\n",
    "\n",
    "# Define and conditionally reset global model variables\n",
    "global global_model\n",
    "global training_results\n",
    "global current_episode\n",
    "\n",
    "if 'global_model' in globals():\n",
    "    try:\n",
    "        if global_model is not None:\n",
    "            del global_model \n",
    "            print(\"Previous SWMModel instance deleted to clear scheduler.\")\n",
    "    except NameError:\n",
    "        pass \n",
    "\n",
    "# Initialize global variables (Must be defined before Cell 7)\n",
    "global_model = SWMModel(RL_CONFIG)\n",
    "training_results = []\n",
    "current_episode = 0\n",
    "\n",
    "print(f\"Model initialized. Target training episodes: {MAX_EPISODES}\")\n",
    "print(f\"Total actions in the selected RL space: {len(HYBRID_ACTIONS)}\")\n",
    "\n",
    "\n",
    "# --- ITERATION CONTROL FUNCTION (Remains defined here) ---\n",
    "def run_single_episode(model, episode_number):\n",
    "    \"\"\"Runs one full episode (4 quarters) of the ABM simulation and updates the RL Q-table.\"\"\"\n",
    "    \n",
    "    if episode_number >= MAX_EPISODES:\n",
    "        return None, True\n",
    "\n",
    "    # Decay Epsilon \n",
    "    model.rl_agent.epsilon = max(\n",
    "        model.rl_agent.epsilon * RL_CONFIG['epsilon_decay'],\n",
    "        RL_CONFIG['epsilon_min']\n",
    "    )\n",
    "    \n",
    "    # Run 4 Quarterly Steps (ABM Simulation)\n",
    "    for _ in range(STEPS_PER_EPISODE):\n",
    "        model.step() \n",
    "\n",
    "    # Record Results\n",
    "    df = model.datacollector.get_model_vars_dataframe()\n",
    "    episode_data = df.iloc[-1]\n",
    "    \n",
    "    return episode_data, False\n",
    "\n",
    "# Output check: Status message for readiness\n",
    "print(\"\\nReady to begin full training loop in the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e508a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop for 5000 episodes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66930347869f4ad3925b44ba5f3780a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Status] Episode 100 | Avg Comp: 0.9551 | Epsilon: 0.3660\n",
      "\n",
      "[Status] Episode 200 | Avg Comp: 0.9517 | Epsilon: 0.1340\n",
      "\n",
      "[Status] Episode 300 | Avg Comp: 0.9591 | Epsilon: 0.0490\n",
      "\n",
      "[Status] Episode 400 | Avg Comp: 0.9594 | Epsilon: 0.0180\n",
      "\n",
      "[Status] Episode 500 | Avg Comp: 0.9600 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 600 | Avg Comp: 0.9600 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 700 | Avg Comp: 0.9600 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 800 | Avg Comp: 0.9610 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 900 | Avg Comp: 0.9612 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1000 | Avg Comp: 0.9614 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1100 | Avg Comp: 0.9614 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1200 | Avg Comp: 0.9614 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1300 | Avg Comp: 0.9622 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1400 | Avg Comp: 0.9629 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1500 | Avg Comp: 0.9629 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1600 | Avg Comp: 0.9631 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1700 | Avg Comp: 0.9639 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1800 | Avg Comp: 0.9639 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 1900 | Avg Comp: 0.9647 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2000 | Avg Comp: 0.9655 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2100 | Avg Comp: 0.9661 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2200 | Avg Comp: 0.9668 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2300 | Avg Comp: 0.9674 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2400 | Avg Comp: 0.9686 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2500 | Avg Comp: 0.9688 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2600 | Avg Comp: 0.9688 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2700 | Avg Comp: 0.9697 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2800 | Avg Comp: 0.9697 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 2900 | Avg Comp: 0.9708 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3000 | Avg Comp: 0.9712 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3100 | Avg Comp: 0.9721 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3200 | Avg Comp: 0.9721 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3300 | Avg Comp: 0.9721 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3400 | Avg Comp: 0.9728 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3500 | Avg Comp: 0.9728 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3600 | Avg Comp: 0.9728 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3700 | Avg Comp: 0.9736 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3800 | Avg Comp: 0.9853 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 3900 | Avg Comp: 0.9738 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4000 | Avg Comp: 0.9751 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4100 | Avg Comp: 0.9757 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4200 | Avg Comp: 0.9760 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4300 | Avg Comp: 0.9768 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4400 | Avg Comp: 0.9777 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4500 | Avg Comp: 0.9780 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4600 | Avg Comp: 0.9784 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4700 | Avg Comp: 0.9784 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4800 | Avg Comp: 0.9784 | Epsilon: 0.0100\n",
      "\n",
      "[Status] Episode 4900 | Avg Comp: 0.9801 | Epsilon: 0.0100\n",
      "\n",
      "--- FULL TRAINING COMPLETE (5000 Episodes Run) ---\n",
      "Proceed to Cell 8 for final analysis and plotting.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Full Automatic Training Loop ðŸš€ (Runs 5000 Episodes)\n",
    "\n",
    "# Import tqdm for a nice progress bar\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "# Check if the model is initialized (should be defined by Cell 6)\n",
    "if 'global_model' not in globals() or global_model is None:\n",
    "    print(\"ERROR: Model not initialized. Please run Cell 6 first.\")\n",
    "else:\n",
    "    print(f\"Starting training loop for {MAX_EPISODES} episodes...\")\n",
    "\n",
    "    # Start the loop from the current episode count up to MAX_EPISODES\n",
    "    start_episode = current_episode\n",
    "    \n",
    "    # Use tqdm to create the progress bar\n",
    "    for episode in tqdm(range(start_episode + 1, MAX_EPISODES + 1), initial=start_episode, total=MAX_EPISODES, desc=\"Training Progress\"):\n",
    "        \n",
    "        # 1. Update the global counter\n",
    "        current_episode = episode\n",
    "        \n",
    "        # 2. Run the episode\n",
    "        new_data, is_finished = run_single_episode(global_model, current_episode)\n",
    "\n",
    "        if is_finished:\n",
    "            break\n",
    "        \n",
    "        # 3. Store results\n",
    "        training_results.append(new_data)\n",
    "        \n",
    "        # 4. Optional: Print status periodically (e.g., every 100 episodes)\n",
    "        if episode % 100 == 0:\n",
    "            avg_compliance = new_data['AvgCompliance']\n",
    "            epsilon = global_model.rl_agent.epsilon\n",
    "            print(f\"\\n[Status] Episode {episode} | Avg Comp: {avg_compliance:.4f} | Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n--- FULL TRAINING COMPLETE ({current_episode} Episodes Run) ---\")\n",
    "    print(\"Proceed to Cell 8 for final analysis and plotting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
