{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641f70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ABM Framework (Mesa)\n",
    "from mesa import Agent, Model\n",
    "# FIX: Use the 'schedule' submodule, which is the correct path for your installation\n",
    "from mesa.time import RandomActivation \n",
    "from mesa.datacollection import DataCollector\n",
    "\n",
    "\n",
    "# --- GLOBAL MODEL PARAMETERS (Synthesized from Thesis Data) ---\n",
    "\n",
    "# Contextual Parameters (from Appendix E & Chapter 3.2)\n",
    "ANNUAL_SWM_BUDGET = 1_500_000 # ₱1,500,000\n",
    "QUARTERLY_BUDGET = ANNUAL_SWM_BUDGET / 4 # ₱375,000\n",
    "NUM_BARANGAYS = 7\n",
    "NUM_EPISODES = 10000 # Number of training runs for the RL agent\n",
    "STEPS_PER_EPISODE = 4 # 4 quarters per year (episode)\n",
    "\n",
    "# Barangay Data (Simplified placeholder for 7 barangays)\n",
    "# Keys are names; values are tuples (Total_Households, Local_SWM_Budget, Initial_Compliance_Estimate)\n",
    "BARANGAY_DATA = {\n",
    "    \"LianganEast\": (608, 30000, 0.65), # 60-70% from interview, used for calibration\n",
    "    \"Poblacion\": (700, 40000, 0.10),\n",
    "    \"Esperanza\": (550, 25000, 0.08),\n",
    "    \"Binuni\": (400, 20000, 0.05), # Likely lower for inland/hard-to-reach areas\n",
    "    \"Demologan\": (450, 22000, 0.07),\n",
    "    \"Mati\": (500, 30000, 0.12),\n",
    "    \"Babalaya\": (350, 18000, 0.09),\n",
    "}\n",
    "\n",
    "# Cost Parameter Estimation (Chapter 3.2.4)\n",
    "# Placeholder costs derived from proxy estimation (e.g., regional minimum wage for enforcers)\n",
    "COST_PER_ENFORCER_QUARTER = 30000 # Estimated salary/operations for one enforcer (FTE)\n",
    "COST_PER_IEC_INTENSITY_UNIT = 10000 # Estimated cost of 1 week of radio spots/materials\n",
    "PAYOUT_PER_COMPLIANT_HOUSEHOLD = 50 # ₱50 incentive (goods/rice)\n",
    "\n",
    "# RL Reward Weights (Chapter 3.4.3) - Tuned during validation\n",
    "ALPHA_COMPLIANCE = 10.0 # High weight for compliance (primary goal)\n",
    "BETA_COST = 0.00001 # Small cost penalty to encourage efficiency\n",
    "GAMMA_DEFICIT = 50.0 # Large penalty for exceeding the budget (fiscal discipline)\n",
    "\n",
    "# TPB Parameter Defaults (wA, wSN, wPBC) (Chapter 3.2.1)\n",
    "# These are synthesized from literature and calibrated to match 10% LGU estimate.\n",
    "# Note: Income/Education weights are implicit in how UPolicy(I) and 'A' are calculated.\n",
    "TPB_WEIGHTS = {\n",
    "    'wA': 0.45,  # Attitude\n",
    "    'wSN': 0.35, # Subjective Norms (High social influence in PH context)\n",
    "    'wPBC': 0.20 # Perceived Behavioral Control\n",
    "}\n",
    "\n",
    "# Psychological Reactance Threshold (Chapter 3.2.1)\n",
    "REACTANCE_THRESHOLD = 0.25 # If >25% of neighbors are fined, Attitude (A) is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef277b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseholdAgent(Agent):\n",
    "    \"\"\"\n",
    "    Represents a household whose decision to segregate is based on the \n",
    "    Theory of Planned Behavior (TPB) and LGU policy utility.\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, initial_income, initial_edu_level, barangay_id):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.barangay_id = barangay_id\n",
    "        self.income = initial_income # Modulates sensitivity to financial policies\n",
    "        self.education = initial_edu_level\n",
    "        self.is_compliant = 0 # 1 if compliant, 0 otherwise\n",
    "        \n",
    "        # Internal TPB Constructs (A, SN, PBC - initialized based on synthetic data)\n",
    "        self.attitude = random.uniform(0.3, 0.7)\n",
    "        self.subj_norm = random.uniform(0.2, 0.6)\n",
    "        self.perc_b_control = random.uniform(0.4, 0.8)\n",
    "\n",
    "    def calculate_utility(self, fine_magnitude, incentive_payout, neighborhood_compliance):\n",
    "        \"\"\"\n",
    "        USegregate = (wA*A + wSN*SN + wPBC*PBC) + UPolicy(I) + epsilon\n",
    "        (Chapter 2.3.1, Fig. 3.1)\n",
    "        \"\"\"\n",
    "        wA, wSN, wPBC = TPB_WEIGHTS['wA'], TPB_WEIGHTS['wSN'], TPB_WEIGHTS['wPBC']\n",
    "        \n",
    "        # 1. Psychological Component (Internal Decision Drivers)\n",
    "        Psych_Utility = (wA * self.attitude) + \\\n",
    "                        (wSN * self.subj_norm) + \\\n",
    "                        (wPBC * self.perc_b_control)\n",
    "        \n",
    "        # 2. Policy Component (Objective Cost/Benefit) - UPolicy(I)\n",
    "        # Note: Sensitivity is modulated by income (I)\n",
    "        # Low income (I) -> higher sensitivity to fine/incentive magnitude\n",
    "        income_sensitivity = 1.0 - (self.income / self.model.MAX_INCOME_PROXY) \n",
    "\n",
    "        # Fine disutility (negative) - if not compliant, assume risk of fine\n",
    "        # Fine disutility is higher for low-income households\n",
    "        Fine_Disutility = -fine_magnitude * income_sensitivity * self.model.ENFORCEMENT_PERCEPTION_RATE \n",
    "        \n",
    "        # Incentive utility (positive) - perceived reward for compliance\n",
    "        Incentive_Utility = incentive_payout * (1.0 - income_sensitivity) # Less sensitive if high income\n",
    "        \n",
    "        Policy_Utility = Fine_Disutility + Incentive_Utility\n",
    "\n",
    "        # 3. Stochastic Component (Epsilon)\n",
    "        epsilon = random.gauss(0, 0.1) # Gaussian noise term\n",
    "\n",
    "        USegregate = Psych_Utility + Policy_Utility + epsilon\n",
    "        return USegregate\n",
    "\n",
    "    def update_tpb_constructs(self, policy_action, neighborhood_fined_rate, avg_compliance):\n",
    "        \"\"\"\n",
    "        Updates A, SN, and PBC based on LGU investment and social observation.\n",
    "        (Chapter 3.3.1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Update Attitude (A) based on IEC and Reactance\n",
    "        # IEC Investment increases A (positive influence)\n",
    "        self.attitude += 0.01 * policy_action['IEC'] * self.model.IEC_EFFECTIVENESS\n",
    "        \n",
    "        # Psychological Reactance (Negative influence)\n",
    "        if neighborhood_fined_rate > REACTANCE_THRESHOLD:\n",
    "            self.attitude -= 0.05 * (neighborhood_fined_rate - REACTANCE_THRESHOLD)\n",
    "            \n",
    "        # 2. Update Subjective Norm (SN) based on observed compliance\n",
    "        # Observed compliance (social influence) increases SN\n",
    "        self.subj_norm = (0.8 * self.subj_norm) + (0.2 * avg_compliance)\n",
    "        \n",
    "        # 3. PBC is updated by the LGU/Barangay's infrastructure provision (Simplified)\n",
    "        # Assuming high IEC/Enforcement indicates LGU dedication, which enhances PBC\n",
    "        self.perc_b_control += 0.005 * (policy_action['IEC'] + policy_action['ENFORCE'])\n",
    "\n",
    "        # Clamp values\n",
    "        self.attitude = np.clip(self.attitude, 0, 1)\n",
    "        self.subj_norm = np.clip(self.subj_norm, 0, 1)\n",
    "        self.perc_b_control = np.clip(self.perc_b_control, 0, 1)\n",
    "\n",
    "    def step(self, fine_magnitude, incentive_payout, neighborhood_compliance):\n",
    "        \"\"\"Decision to segregate or not.\"\"\"\n",
    "        utility = self.calculate_utility(fine_magnitude, incentive_payout, neighborhood_compliance)\n",
    "\n",
    "        # Decision threshold: if utility is positive, the agent segregates (is compliant)\n",
    "        if utility > 0.5: # 0.5 is an arbitrary threshold; can be calibrated\n",
    "            self.is_compliant = 1\n",
    "        else:\n",
    "            self.is_compliant = 0\n",
    "            \n",
    "        return self.is_compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05cfc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarangayAgent(Agent):\n",
    "    \"\"\"\n",
    "    Intermediate implementation layer. Receives LGU funds and manages local \n",
    "    implementation (enforcement, IEC, incentives).\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, name, households, local_budget, initial_compliance):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.name = name\n",
    "        self.households = households # List of HouseholdAgents in this barangay\n",
    "        self.local_budget = local_budget # Local SWM budget (e.g., ₱30k for Liangan East)\n",
    "        self.current_compliance_rate = initial_compliance\n",
    "        self.last_enforcement_fined_count = 0\n",
    "        self.policy_allocation = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "        \n",
    "    def aggregate_and_report(self):\n",
    "        \"\"\"Aggregates household compliance and calculates local policy effects.\"\"\"\n",
    "        \n",
    "        # 1. Calculate Aggregate Compliance\n",
    "        compliant_count = sum(h.is_compliant for h in self.households)\n",
    "        self.current_compliance_rate = compliant_count / len(self.households)\n",
    "        \n",
    "        # 2. Calculate Enforcement/Fine Effectiveness\n",
    "        # A compliant agent is not fined. A non-compliant agent is fined based on enforcement spending.\n",
    "        non_compliant_count = len(self.households) - compliant_count\n",
    "        \n",
    "        # Enforcement effectiveness is proportional to spending.\n",
    "        enforcement_staff = self.policy_allocation['ENFORCE'] / COST_PER_ENFORCER_QUARTER\n",
    "        \n",
    "        # Probability of being caught/fined (simulated based on staff count)\n",
    "        fined_prob = min(1.0, 0.05 * enforcement_staff) \n",
    "        \n",
    "        # Number of households fined this quarter\n",
    "        fined_count = sum(1 for _ in range(non_compliant_count) if random.random() < fined_prob)\n",
    "        self.last_enforcement_fined_count = fined_count\n",
    "\n",
    "        # Calculate incentive cost and successful enforcement cost\n",
    "        incentive_cost = compliant_count * PAYOUT_PER_COMPLIANT_HOUSEHOLD\n",
    "        enforcement_revenue = fined_count * self.model.LGU_FINE_MAGNITUDE # Revenue from fines\n",
    "        \n",
    "        return self.current_compliance_rate, self.last_enforcement_fined_count, incentive_cost, enforcement_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041575f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGURLAgent(Agent):\n",
    "    \"\"\"\n",
    "    The Municipal LGU agent, which uses Q-learning to determine the optimal \n",
    "    budget allocation strategy (the action At).\n",
    "    \"\"\"\n",
    "    def __init__(self, unique_id, model, rl_config):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.lr = rl_config['learning_rate']\n",
    "        self.gamma = rl_config['discount_factor']\n",
    "        self.epsilon = rl_config['epsilon']\n",
    "        self.action_space = rl_config['action_space'] # The discrete policy options\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.action_space)))\n",
    "        self.last_state = None\n",
    "        self.last_action_idx = None\n",
    "        \n",
    "    def get_state(self, model):\n",
    "        \"\"\"\n",
    "        Defines the State Vector St = [Compliancet, AllocationPrev, BudgetRem, Quartert]\n",
    "        (Chapter 3.4.1)\n",
    "        \"\"\"\n",
    "        # 1. Compliance (7 values)\n",
    "        compliance_vector = tuple(b.current_compliance_rate for b in model.barangays.values())\n",
    "        \n",
    "        # 2. Previous Allocation (Simplified placeholder for 21 values)\n",
    "        # Simplification: Use total spending per lever, not all 21 values, for discrete Q-Learning state\n",
    "        prev_alloc_iec = sum(b.policy_allocation['IEC'] for b in model.barangays.values())\n",
    "        prev_alloc_enforce = sum(b.policy_allocation['ENFORCE'] for b in model.barangays.values())\n",
    "        prev_alloc_incentive = sum(b.policy_allocation['INCENTIVE'] for b in model.barangays.values())\n",
    "        \n",
    "        # 3. Budget Remaining (Discretized)\n",
    "        budget_remaining = model.annual_budget_remaining\n",
    "        budget_bin = int(budget_remaining // 100000) # Discretize in ₱100k bins\n",
    "        \n",
    "        # 4. Current Quarter\n",
    "        quarter = model.schedule.steps % STEPS_PER_EPISODE\n",
    "        \n",
    "        # The state is a tuple for use as a dictionary key in the Q-table\n",
    "        return compliance_vector + (budget_bin, quarter) \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy strategy to select one of the discrete policy options.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: Choose a random discrete action\n",
    "            action_idx = random.randrange(len(self.action_space))\n",
    "        else:\n",
    "            # Exploitation: Choose the best action from Q-table\n",
    "            action_idx = np.argmax(self.q_table[state])\n",
    "            \n",
    "        self.last_state = state\n",
    "        self.last_action_idx = action_idx\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def update_Q(self, new_state, reward):\n",
    "        \"\"\"Q-Learning update rule.\"\"\"\n",
    "        old_value = self.q_table[self.last_state][self.last_action_idx]\n",
    "        next_max = np.max(self.q_table[new_state])\n",
    "        \n",
    "        # Q(S, A) = Q(S, A) + LR * (Reward + Gamma * max(Q(S', a')) - Q(S, A))\n",
    "        new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n",
    "        self.q_table[self.last_state][self.last_action_idx] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc9b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWMModel(Model):\n",
    "    \"\"\"The main Agent-Based Model environment for SWM policy simulation.\"\"\"\n",
    "    def __init__(self, rl_config, fine_magnitude=500):\n",
    "        super().__init__()\n",
    "        self.schedule = RandomActivation(self)\n",
    "        self.running = True\n",
    "        self.MAX_INCOME_PROXY = 50000 # Max income proxy for sensitivity calculation\n",
    "        self.ENFORCEMENT_PERCEPTION_RATE = 0.5 # Placeholder for how often households perceive enforcement\n",
    "        self.IEC_EFFECTIVENESS = 0.05\n",
    "        self.LGU_FINE_MAGNITUDE = fine_magnitude # ₱500 per citation (from Ordinance D)\n",
    "\n",
    "        # RL Parameters\n",
    "        self.rl_agent = LGURLAgent(999, self, rl_config)\n",
    "        self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "        self.quarterly_budget = QUARTERLY_BUDGET\n",
    "        \n",
    "        # Initialization\n",
    "        self.current_id = 0\n",
    "        self.barangays = {}\n",
    "        self.household_agents = []\n",
    "        self._initialize_agents()\n",
    "        \n",
    "        # Data Collector setup\n",
    "        self.datacollector = DataCollector(\n",
    "            model_reporters={\"AvgCompliance\": lambda m: np.mean([b.current_compliance_rate for b in m.barangays.values()]),\n",
    "                             \"TotalCost\": lambda m: m.last_cost,\n",
    "                             \"RLReward\": lambda m: m.last_reward,\n",
    "                             \"BudgetAllocIEC\": lambda m: m.last_alloc['IEC'],\n",
    "                             \"BudgetAllocENFORCE\": lambda m: m.last_alloc['ENFORCE'],\n",
    "                             \"BudgetAllocINCENTIVE\": lambda m: m.last_alloc['INCENTIVE'],\n",
    "                            },\n",
    "            agent_reporters={} # Can be used to track individual household behavior\n",
    "        )\n",
    "        self.last_cost = 0\n",
    "        self.last_reward = 0\n",
    "        self.last_alloc = {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0}\n",
    "\n",
    "    def _initialize_agents(self):\n",
    "        \"\"\"Initializes all Household and Barangay Agents.\"\"\"\n",
    "        for i, (name, data) in enumerate(BARANGAY_DATA.items()):\n",
    "            num_h, local_budget, initial_compliance = data\n",
    "            barangay_agent = BarangayAgent(i, self, name, [], local_budget, initial_compliance)\n",
    "            self.schedule.add(barangay_agent)\n",
    "            self.barangays[name] = barangay_agent\n",
    "            \n",
    "            # Create Household Agents for this barangay\n",
    "            for _ in range(num_h):\n",
    "                self.current_id += 1\n",
    "                # Simplified income/education proxy\n",
    "                income = random.uniform(10000, self.MAX_INCOME_PROXY)\n",
    "                edu_level = random.choice([0, 1, 2]) # Low, Mid, High\n",
    "                h_agent = HouseholdAgent(self.current_id, self, income, edu_level, name)\n",
    "                self.household_agents.append(h_agent)\n",
    "                barangay_agent.households.append(h_agent)\n",
    "                self.schedule.add(h_agent)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Advance the model by one quarter (one RL step).\n",
    "        (Chapter 3.3)\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. RL Agent makes Policy Decision (Action) ---\n",
    "        current_state = self.rl_agent.get_state(self)\n",
    "        policy_vector = self.rl_agent.choose_action(current_state) # 21-D vector\n",
    "        \n",
    "        # Process the 21-D action vector into 7 barangay allocations\n",
    "        total_spending = 0\n",
    "        policy_breakdown = defaultdict(lambda: {'IEC': 0, 'ENFORCE': 0, 'INCENTIVE': 0})\n",
    "        \n",
    "        # The RL action vector is structured: [IEC_B1, E_B1, I_B1, IEC_B2, ...]\n",
    "        for i, b_name in enumerate(BARANGAY_DATA.keys()):\n",
    "            b_agent = self.barangays[b_name]\n",
    "            \n",
    "            # Slice the 21-D vector (3 values per barangay)\n",
    "            alloc_iec = policy_vector[i * 3 + 0]\n",
    "            alloc_enforce = policy_vector[i * 3 + 1]\n",
    "            alloc_incentive = policy_vector[i * 3 + 2]\n",
    "\n",
    "            # Assign allocations to the Barangay Agent for use in the step\n",
    "            b_agent.policy_allocation['IEC'] = alloc_iec\n",
    "            b_agent.policy_allocation['ENFORCE'] = alloc_enforce\n",
    "            b_agent.policy_allocation['INCENTIVE'] = alloc_incentive\n",
    "            \n",
    "            total_spending += alloc_iec + alloc_enforce + alloc_incentive\n",
    "\n",
    "        # --- 2. Household Agents make Segregation Decisions ---\n",
    "        # Run household steps first\n",
    "        self.schedule.step()\n",
    "        \n",
    "        # --- 3. Barangay Agents Aggregate and Update ---\n",
    "        total_compliance = 0\n",
    "        total_households = 0\n",
    "        total_fined = 0\n",
    "        total_incentive_cost = 0\n",
    "        total_enforcement_revenue = 0\n",
    "        \n",
    "        for b_agent in self.barangays.values():\n",
    "            # Get neighborhood context for Household Agent updates\n",
    "            neighborhood_compliance = b_agent.current_compliance_rate\n",
    "            \n",
    "            # Aggregation and Reporting\n",
    "            compliance, fined_count, incentive_cost, enforcement_revenue = b_agent.aggregate_and_report()\n",
    "            \n",
    "            total_compliance += compliance * len(b_agent.households)\n",
    "            total_households += len(b_agent.households)\n",
    "            total_fined += fined_count\n",
    "            total_incentive_cost += incentive_cost\n",
    "            total_enforcement_revenue += enforcement_revenue\n",
    "\n",
    "            # Update Household TPB constructs based on LGU action\n",
    "            fined_rate = fined_count / len(b_agent.households) if len(b_agent.households) > 0 else 0\n",
    "            for h_agent in b_agent.households:\n",
    "                h_agent.update_tpb_constructs(b_agent.policy_allocation, fined_rate, neighborhood_compliance)\n",
    "\n",
    "        # --- 4. RL Agent Calculates Reward and Updates Q-Table ---\n",
    "        avg_compliance = total_compliance / total_households\n",
    "        \n",
    "        # Total cost is LGU spending plus the dynamic incentive payout\n",
    "        actual_total_cost = total_spending + total_incentive_cost \n",
    "        budget_deficit = max(0, actual_total_cost - self.quarterly_budget)\n",
    "        \n",
    "        # Reward Function (Rt = α*Compliance - β*TotalCost - γ*BudgetDeficit)\n",
    "        reward = (ALPHA_COMPLIANCE * avg_compliance) - \\\n",
    "                 (BETA_COST * actual_total_cost) - \\\n",
    "                 (GAMMA_DEFICIT * budget_deficit)\n",
    "        \n",
    "        new_state = self.rl_agent.get_state(self)\n",
    "        self.rl_agent.update_Q(new_state, reward)\n",
    "        \n",
    "        # Update annual budget remaining for the next step's state\n",
    "        self.annual_budget_remaining -= actual_total_cost\n",
    "        \n",
    "        # Reset budget and end episode if the year is over\n",
    "        if self.schedule.steps % STEPS_PER_EPISODE == 0:\n",
    "            self.annual_budget_remaining = ANNUAL_SWM_BUDGET\n",
    "            \n",
    "        # Store metrics for data collection\n",
    "        self.last_cost = actual_total_cost\n",
    "        self.last_reward = reward\n",
    "        self.last_alloc = {'IEC': sum(b.policy_allocation['IEC'] for b in self.barangays.values()),\n",
    "                           'ENFORCE': sum(b.policy_allocation['ENFORCE'] for b in self.barangays.values()),\n",
    "                           'INCENTIVE': sum(b.policy_allocation['INCENTIVE'] for b in self.barangays.values())}\n",
    "\n",
    "        self.datacollector.collect(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e508a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total actions in the selected RL space: 27\n"
     ]
    }
   ],
   "source": [
    "# --- RL Action Space Definition (Chapter 3.4.2) ---\n",
    "\n",
    "# For discrete Q-Learning, the 21-D action space must be manageable. \n",
    "# We simplify the decision to allocation across 3 spending levels (Low, Med, High)\n",
    "# for the 3 policy levers, applied uniformly to all 7 barangays initially,\n",
    "# and then expanding the action space for the final analysis.\n",
    "\n",
    "# Policy Levers and Spending Granularity (total quarterly budget ₱375,000)\n",
    "# Low: ₱50k, Med: ₱125k, High: ₱200k (These are illustrative spending levels)\n",
    "\n",
    "# Function to generate a simplified 21-D action vector for a single scenario type.\n",
    "def generate_scenario_actions(iec_enabled, enforce_enabled, incentive_enabled):\n",
    "    \"\"\"Generates the 21-D action vectors for the specified regime.\"\"\"\n",
    "    # Simplified total quarterly spending options (to keep Q-table size small for example)\n",
    "    spending_levels = [0, 50000, 100000] \n",
    "    \n",
    "    actions = []\n",
    "    \n",
    "    # Iterate over all possible combinations of spending across 3 levers (IEC, ENFORCE, INCENTIVE)\n",
    "    for s_iec in spending_levels:\n",
    "        if not iec_enabled and s_iec > 0: continue\n",
    "        for s_enforce in spending_levels:\n",
    "            if not enforce_enabled and s_enforce > 0: continue\n",
    "            for s_incentive in spending_levels:\n",
    "                if not incentive_enabled and s_incentive > 0: continue\n",
    "                \n",
    "                total_alloc = s_iec + s_enforce + s_incentive\n",
    "                if total_alloc <= QUARTERLY_BUDGET: # Respect budget constraint\n",
    "                    \n",
    "                    # Distribute this total allocation uniformly across 7 barangays\n",
    "                    # In the final model, this should be a 21-D vector, but for initial training, we simplify.\n",
    "                    \n",
    "                    # The actual 21-D vector: [IEC_B1, E_B1, I_B1, ..., IEC_B7, E_B7, I_B7]\n",
    "                    action_vector = []\n",
    "                    for _ in range(NUM_BARANGAYS):\n",
    "                        action_vector.extend([s_iec/NUM_BARANGAYS, s_enforce/NUM_BARANGAYS, s_incentive/NUM_BARANGAYS])\n",
    "                    \n",
    "                    actions.append(tuple(action_vector)) # Must be a tuple for the Q-table key\n",
    "                    \n",
    "    return actions\n",
    "\n",
    "# 1. Pure Incentive Regime (IEC + Incentive only)\n",
    "# Disabling ENFORCE for all 7 barangays\n",
    "PURE_INCENTIVE_ACTIONS = generate_scenario_actions(True, False, True)\n",
    "\n",
    "# 2. Pure Penalty Regime (IEC + Enforcement only)\n",
    "# Disabling INCENTIVE for all 7 barangays\n",
    "PURE_PENALTY_ACTIONS = generate_scenario_actions(True, True, False)\n",
    "\n",
    "# 3. Hybrid Regime (IEC + Enforcement + Incentive)\n",
    "# All levers are active\n",
    "HYBRID_ACTIONS = generate_scenario_actions(True, True, True)\n",
    "\n",
    "# Select the regime to train for in this example run\n",
    "RL_ACTION_SPACE = HYBRID_ACTIONS \n",
    "\n",
    "RL_CONFIG = {\n",
    "    'learning_rate': 0.1,\n",
    "    'discount_factor': 0.9,\n",
    "    'epsilon': 1.0, # Start with high exploration\n",
    "    'epsilon_decay': 0.9995,\n",
    "    'epsilon_min': 0.01,\n",
    "    'action_space': RL_ACTION_SPACE\n",
    "}\n",
    "\n",
    "print(f\"Total actions in the selected RL space: {len(RL_ACTION_SPACE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f61bbda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, results\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Run the training (Set a lower number for faster testing)\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# trained_model, training_results = run_rl_training(RL_CONFIG, num_episodes=5000)\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Example run with a reduced number of episodes for the notebook demonstration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m trained_model, training_results = \u001b[43mrun_rl_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRL_CONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     38\u001b[39m results_df = pd.DataFrame(training_results)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Training Complete ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun_rl_training\u001b[39m\u001b[34m(model_config, num_episodes)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_rl_training\u001b[39m(model_config, num_episodes):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the full RL training and returns the collected data.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model = \u001b[43mSWMModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     results = []\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting RL Training...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSWMModel.__init__\u001b[39m\u001b[34m(self, rl_config, fine_magnitude)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.LGU_FINE_MAGNITUDE = fine_magnitude \u001b[38;5;66;03m# ₱500 per citation (from Ordinance D)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# RL Parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mself\u001b[39m.rl_agent = \u001b[43mLGURLAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrl_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.annual_budget_remaining = ANNUAL_SWM_BUDGET\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.quarterly_budget = QUARTERLY_BUDGET\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mLGURLAgent.__init__\u001b[39m\u001b[34m(self, unique_id, model, rl_config)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unique_id, model, rl_config):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr = rl_config[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.gamma = rl_config[\u001b[33m'\u001b[39m\u001b[33mdiscount_factor\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jemar John\\Python Projects\\.venv\\Lib\\site-packages\\mesa\\agent.py:63\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model, *args, **kwargs)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mTypeError\u001b[39m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "# --- SIMULATION TRAINING LOOP ---\n",
    "\n",
    "def run_rl_training(model_config, num_episodes):\n",
    "    \"\"\"Runs the full RL training and returns the collected data.\"\"\"\n",
    "    \n",
    "    model = SWMModel(model_config)\n",
    "    results = []\n",
    "    \n",
    "    print(\"Starting RL Training...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        model.annual_budget_remaining = ANNUAL_SWM_BUDGET # Reset budget annually\n",
    "        \n",
    "        # Decay epsilon to shift from exploration to exploitation\n",
    "        model.rl_agent.epsilon = max(model.rl_agent.epsilon * model_config['epsilon_decay'], \n",
    "                                     model_config['epsilon_min'])\n",
    "        \n",
    "        # Run one 'episode' (4 quarters/steps)\n",
    "        for _ in range(STEPS_PER_EPISODE):\n",
    "            model.step()\n",
    "        \n",
    "        # Record final metrics for the episode\n",
    "        df = model.datacollector.get_model_vars_dataframe()\n",
    "        results.append(df.iloc[-1]) # Store the end-of-year state\n",
    "        \n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} completed. Current Epsilon: {model.rl_agent.epsilon:.4f}\")\n",
    "            \n",
    "    return model, results\n",
    "\n",
    "\n",
    "# Run the training (Set a lower number for faster testing)\n",
    "# trained_model, training_results = run_rl_training(RL_CONFIG, num_episodes=5000)\n",
    "\n",
    "# Example run with a reduced number of episodes for the notebook demonstration\n",
    "trained_model, training_results = run_rl_training(RL_CONFIG, num_episodes=500) \n",
    "\n",
    "results_df = pd.DataFrame(training_results)\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- PLOTTING AND ANALYSIS (Chapter 3.5.3) ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Compliance Performance (Primary Metric)\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Use rolling average to smooth the learning curve\n",
    "results_df['AvgCompliance_Smoothed'] = results_df['AvgCompliance'].rolling(window=50).mean() \n",
    "plt.plot(results_df['AvgCompliance_Smoothed'])\n",
    "plt.title('Maximum Sustainable Compliance (Smoothed Avg Compliance Rate)')\n",
    "plt.xlabel('Episode (Simulated Year)')\n",
    "plt.ylabel('Average Compliance Rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Optimal Resource Allocation (Key Output)\n",
    "final_alloc_mean = results_df.iloc[-200:].mean() # Average of the last 200 episodes (stable policy)\n",
    "total_mean_spending = final_alloc_mean['BudgetAllocIEC'] + final_alloc_mean['BudgetAllocENFORCE'] + final_alloc_mean['BudgetAllocINCENTIVE']\n",
    "\n",
    "allocations = {\n",
    "    'IEC': final_alloc_mean['BudgetAllocIEC'],\n",
    "    'Enforcement': final_alloc_mean['BudgetAllocENFORCE'],\n",
    "    'Incentives': final_alloc_mean['BudgetAllocINCENTIVE']\n",
    "}\n",
    "\n",
    "print(f\"\\nOptimal Budget Allocation (Average of last 200 quarters): ₱{total_mean_spending:,.2f}\")\n",
    "for policy, cost in allocations.items():\n",
    "    print(f\"- {policy}: ₱{cost:,.2f} ({cost/total_mean_spending:.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
